import io
import re
from typing import Optional

import imageio
import numpy
from ai2thor.controller import Controller
from ai2thor.server import Event
from openai import OpenAI
from PIL import Image, ImageDraw, ImageFont
from pydantic import BaseModel

SYSTEM_PROMPT = """
You are an embodied agent that receives images and acts in a 3D simulated environment. 
You can move around and interact with objects. These are the actions at your disposal:
- MoveAhead: agent moves ahead one step
- MoveBack: agent moves back one step
- MoveLeft: agent moves to the left one step
- MoveRight: agent moves to the right one step
- RotateRight(degrees): agent rotates to the right by a given number of degrees
- RotateLeft(degrees): agent rotates to the left by a given number of degrees
- LookUp: agent looks up
- LookDown: agent looks down
You can also use manipulation actions which require you to specify the object name of a visible object:
- OpenObject(<object name>): agent opens the object
- CloseObject(<object name>): agent closes the object
- PickupObject(<object name>): agent picks up the object and places it in the inventory
- PutObject(<receptacle name>): the agent has an object in the inventory and places it in the receptacle 
- DropObject(<object name>): the agent drops the object
- ToggleObjectOn(<object name>): the agent toggles the object on
- ToggleObjectOff(<object name>): the agent toggles the object off
- SliceObject(<object name>): the agent slices the object (requires a knife)
If you generate an action, start your response with the tag `[Action]` and follow the format of the
action. Never put quotes around the object name. For example: [Action] OpenObject(Fridge)",
"""

client = OpenAI(base_url="http://localhost:11434/v1", api_key="ollama")


class AgentResponse(BaseModel):
    """
    Base class for agent responses. Provides a method to convert the response
    to a dictionary, filtering out None values.
    """

    def to_dict(self) -> dict:
        """
        Converts the response object to a dictionary, excluding keys with None values.

        Returns:
            dict: A dictionary representation of the response.
        """
        current_dict = self.model_dump(mode="python")
        return {key: value for key, value in current_dict.items() if value is not None}


class AgentTextualResponse(AgentResponse):
    """
    Represents a textual response from the agent.

    Attributes:
        response (str): The textual response generated by the agent.
    """

    response: str


class AgentActionResponse(AgentResponse):
    """
    Represents an action response from the agent.

    Attributes:
        action (str): The action to be performed by the agent.
        objectId (Optional[str]): The ID of the object involved in the action, if any.
        degrees (Optional[float]): The degrees of rotation, if applicable.
    """

    action: str
    objectId: Optional[str] = None
    degrees: Optional[float] = None


class LLMClient:
    """
    Client for interacting with a language model. Handles communication with the model
    and processes responses.

    Attributes:
        model_name (str): The name of the language model to use.
        history (list): A history of interactions with the model.
    """

    def __init__(self, model_name: str = "qwen2.5:1.5b") -> None:
        """
        Initializes the LLMClient with a specified model name.

        Args:
            model_name (str): The name of the language model to use.
        """
        self.model_name = model_name
        self.history = []

    def reset(self) -> None:
        """
        Resets the interaction history with the model.
        """
        print("Model history reset...")
        self.history = []

    def _image_to_bytes(self, image: Image.Image) -> bytes:
        """
        Converts a PIL Image to a byte array in PNG format.

        Args:
            image (Image): The image to convert.

        Returns:
            bytes: The byte array representation of the image.
        """
        img_byte_arr = io.BytesIO()
        image.save(img_byte_arr, format="PNG")
        return img_byte_arr.getvalue()

    def _get_object_id(
        self, raw_object_id: str, env_observation: Event
    ) -> Optional[str]:
        """
        Retrieves the object ID of a visible object based on a partial or raw object ID.

        Args:
            raw_object_id (str): The raw object ID to search for.
            env_observation: The environment observation containing metadata.

        Returns:
            Optional[str]: The matching object ID, or None if no match is found.
        """
        visible_objects_ids = [
            o["objectId"] for o in env_observation.metadata["objects"] if o["visible"]
        ]
        object_id = None

        for visible_object_id in visible_objects_ids:
            if raw_object_id.lower() in visible_object_id.lower():
                object_id = visible_object_id
                print("Found object ID: %s", object_id)
                break

        return object_id
    
    
    def postprocess_response(
        self, env_observation: Event, response: str
    ) -> AgentResponse:
        """
        Processes the raw response from the language model and converts it into
        an appropriate AgentResponse object.

        Args:
            env_observation: The environment observation containing metadata.
            response (str): The raw response from the language model.

        Returns:
            AgentResponse: The processed response as an action or textual response.
        """
        if response.startswith("[Action]"):
            raw_action = response.replace("[Action]", "").strip()

            is_interaction_action = any(
                raw_action.startswith(interaction)
                for interaction in [
                    "OpenObject",
                    "CloseObject",
                    "PickupObject",
                    "PutObject",
                    "DropObject",
                    "ToggleObject",
                    "SliceObject",
                ]
            )

            if is_interaction_action:
                # create regex to extract object id and action name based on template
                # e.g. OpenObject(<object_id>)

                pattern = r"(\w+)\((\w+).*\)"
                match = re.match(pattern, raw_action)

                if match:
                    action = match.group(1)
                    raw_object_id = match.group(2).strip()
                    print(f"raw_object_id: {raw_object_id}")

                    object_id = self._get_object_id(raw_object_id, env_observation)
                    if object_id is None:
                        return AgentActionResponse(
                            action="No visible object with that id."
                        )
                    return AgentActionResponse(action=action, objectId=object_id)

            # now extract the first part only
            main_command = raw_action.split("(")[0]
            if main_command in ("RotateRight", "RotateLeft"):
                # attempt to parse the degrees from the raw action
                degrees = None
                try:
                    degrees = float(raw_action.split("(")[1].replace(")", ""))
                except:
                    degrees = 30.0
                # if degrees is not provided, default to 30.0
                return AgentActionResponse(action=main_command, degrees=degrees)
            return AgentActionResponse(action=main_command)

        else:
            verbal_response = response.replace("[Say]", "").strip()
            return AgentTextualResponse(response=verbal_response)

    def act(self, env_observation: Event, language_input: str) -> AgentResponse:
        """
        Sends a language input to the model and processes the response to generate
        an action or textual response.

        Args:
            env_observation: The environment observation containing metadata.
            language_input (str): The language input provided by the user.

        Returns:
            AgentResponse: The processed response as an action or textual response.
        """
        visible_objects = [
            o["objectId"] for o in env_observation.metadata["objects"] if o["visible"]
        ]
        objects_str = "\n".join(visible_objects)

        try:
            completion = client.chat.completions.create(
                temperature=0,
                model=self.model_name,
                messages=[
                    {"role": "system", "content": SYSTEM_PROMPT},
                    {
                        "role": "user",
                        "content": f"Visible objects: {objects_str}",
                    },
                    {
                        "role": "user",
                        "content": language_input,
                    },
                ],
            )

            raw_response = completion.choices[0].message.content
            print(f"Raw response: {raw_response}")
            pattern = r"\[Action\] (\w+):? (\w+)"
            match = re.search(pattern,raw_response)
            if match:
                    action = match.group(1)
                    object_name = match.group(2).strip('\"').strip(")")
    
                    standardized_response=f"[Action] {action}({object_name})"
                    print(f"Standardized raw response: {standardized_response}")
                    return self.postprocess_response(env_observation, standardized_response) 
        except Exception as e:
                    print(f"Error: {e}")

        return self.postprocess_response(env_observation, raw_response)


def render_text_on_image(language_instruction: str, width: int, height: int) -> None:
    # Render the language instruction as an image
    font_size = 35
    try:
        # Create a blank image with white background
        text_image = Image.new("RGB", (width, height), "white")
        draw = ImageDraw.Draw(text_image)

        # Load a default font
        try:
            font = ImageFont.truetype("arial.ttf", font_size)
        except IOError:
            font = ImageFont.load_default()

        # Calculate text position to center it
        text_width = draw.textlength(language_instruction, font=font)
        text_height = font_size

        text_position = (
            (text_image.width - text_width) // 2,
            (text_image.height - text_height) // 2,
        )

        # Draw the text on the image
        draw.text(text_position, language_instruction, fill="black", font=font)

        return text_image
    except Exception as e:
        print(f"Error rendering text as image: {e}")


def main() -> None:
    """
    Main function to initialize the environment, interact with the language model,
    and process user instructions in a loop. Generates a video of the agent's actions.
    """
    # Initialize the AI2-THOR controller which will be used to interact with the environment
    # The controller is responsible for rendering the environment and sending actions to it
    # The controller is initialized with a specific scene and a set of parameters
    controller = Controller(
        quality="Medium", renderDepthImage=False, width=640, height=640
    )

    # The controller can be reset to a specific scene. To know which scenes are available,
    # we can use the ithor_scenes method. This method allows us to filter the scenes based on
    # different categories such as kitchens, living rooms, bedrooms, and bathrooms.
    all_scenes = controller.ithor_scenes(
        include_kitchens=True,
        include_living_rooms=False,
        include_bedrooms=False,
        include_bathrooms=False,
    )

    print("------")
    print("Available scenes:")
    for i, scene in enumerate(all_scenes):
        print(f"{i}: {scene}")
    print("------")

    # For this example, we will use the first scene in the list of available scenes.
    # You can change this to any other scene by changing the index.
    # https://ai2thor.allenai.org/ithor/documentation/scenes
    kitchen_scene = all_scenes[0]
    controller.reset(
        scene=kitchen_scene,
    )

    # The LLM client is responsible for interacting with the language model.
    # It is initialized with a specific model name. You can change this to any other model from
    # Ollama
    # https://ollama.com/models
    # The model name is passed as a string. Make sure that you "pull" the model first using
    # ollama pull <model_name>
    # For this example, we will use the "qwen2.5:1.5b" model.
    model_to_use = "qwen2.5:1.5b"
    client = LLMClient(model_name=model_to_use)

    frames = []
    # The first step is to initialize the environment. This is done by calling the step method
    # on the controller. The step method takes a string as an argument which specifies the action
    # to be performed. In this case, we are initializing the environment.
    # The step method returns an event object which contains information about the current state
    # of the environment.
    event = controller.step("Initialize")
    done = False

    while not done:
        image = Image.fromarray(event.frame)
        frames.append(image)
        language_instruction = input("Enter instruction (enter CLOSE or x to exit): ")
        if language_instruction == "CLOSE" or language_instruction == "x":
            print("Interrupting task...")
            break

        # Render the language instruction as an image
        text_image = render_text_on_image(language_instruction, 640, 640)
        frames.append(text_image)

        print("Processing instruction...")
        response = client.act(event, language_instruction)
        if isinstance(response, AgentTextualResponse):
            print(f"Generated response: {response.response}")
        else:
            print(f"Generated action: {response}")
            try:
                event = controller.step(**response.to_dict(), forceAction=True)
                # forces the GUI to update
                controller.step("NoOp")
            except Exception as e:
                print(f"Unable to execute action due to an error: {e}")

    # Encode all frames into a mp4 video.
    video_path = "rollout.mp4"
    imageio.mimsave(str(video_path), numpy.stack(frames), fps=1, codec="libx264")

    print(f"Video of the evaluation is available in '{video_path}'.")


if __name__ == "__main__":
    main()
